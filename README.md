# ğŸ¯ XGBoost Classification Project

## ğŸ“– Description

This project demonstrates XGBoost (Extreme Gradient Boosting) for binary classification using Python.
It includes EDA, visualization, preprocessing, model training, hyperparameter tuning, and deployment with Streamlit + Pickle.

## ğŸ“‚ Dataset

File: WA_Fn-UseC_-Telco-Customer-Churn.csv (Telco Customer Churn dataset)

Features: Customer demographics, account info, services

Target: Churn (Yes / No)

Size: ~7,043 rows

## âš™ï¸ Methods

EDA (Exploratory Data Analysis): Missing values, distributions, correlation heatmap

Preprocessing: Label encoding, feature scaling, train-test split

Model: XGBoostClassifier with hyperparameter tuning via GridSearchCV

Evaluation Metrics: Accuracy, Precision, Recall, F1-score, ROC-AUC

Deployment: Streamlit app for interactive prediction + Pickle for model saving

## ğŸ“Š Results

âœ… Best Parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 100}

âœ… Accuracy: 85.3%

âœ… ROC-AUC: 0.81

âš ï¸ Precision & Recall show imbalance (low recall = many false negatives)

âœ… Confusion Matrix shows good classification for non-churn, but churn prediction is harder

## ğŸ“¦ Requirements

Install dependencies with:

pip install pandas numpy matplotlib seaborn scikit-learn xgboost streamlit


## Run Streamlit app:

streamlit run app.py

## ğŸ“ˆ Conclusion

XGBoost is highly effective for churn prediction with good accuracy and ROC-AUC

However, recall is low â†’ the model misses some actual churn cases

Feature importance analysis helps understand key drivers of churn

## ğŸš€ Future Improvements

ğŸ”¹ Handle class imbalance with SMOTE or class weights

ğŸ”¹ Feature engineering (interaction features, contract length categories, etc.)

ğŸ”¹ Try advanced models (LightGBM, CatBoost) for comparison

ğŸ”¹ Deploy with Flask or FastAPI as a production API
